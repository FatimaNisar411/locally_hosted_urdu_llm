import chromadb
import requests
from collections import Counter
import time
def get_chroma():
    # Use the existing collection in db folder
    # No need to specify embedding function when getting - it's stored with the collection
    client = chromadb.PersistentClient(path="db")
    collection = client.get_collection(name="tashreeh_chroma_bgem3")
    return collection




collection = get_chroma()
# ------------------------------------------------
# 1. Detect poem title (AUTO-EMBEDDING by Chroma)
# ------------------------------------------------
def detect_poem_title(user_couplet, collection):
    """
    Run similarity search using Chroma's internal embedder.
    Pick the top-1 most similar chunk.
    """
    results = collection.query(
        query_texts=[user_couplet],   # <--- AUTO-EMBEDDING happens here
        n_results=1
    )

    top_metadata = results["metadatas"][0][0]
    poem_title = top_metadata["poem"]

    print(f"ðŸ“Œ Detected poem: {poem_title}")
    return poem_title


# ------------------------------------------------
# 2. Retrieve ALL chunks of that poem
# ------------------------------------------------
def get_poem_chunks(poem_title, collection):
    """
    Pull all chunks where poem_title == requested poem.
    """
    results = collection.get(where={"poem": poem_title})
    


    docs = results["documents"]
    metas = results["metadatas"]

    # Sort by chunk_id if available
    combined = list(zip(docs, metas))
    combined_sorted = sorted(
        combined,
        key=lambda x: int(x[1].get("chunk_id", 0))
    )

    sorted_docs = [c[0] for c in combined_sorted]
    return sorted_docs


# ------------------------------------------------
# 3. Full retrieval pipeline (NO Alif)
# ------------------------------------------------
def retrieve_poem(user_couplet, collection):
    """
    Detect poem â†’ fetch poem chunks â†’ return them.
    """
    poem_title = detect_poem_title(user_couplet, collection)
    chunks = get_poem_chunks(poem_title, collection)

    print(f"ðŸ“š Retrieved {len(chunks)} chunks.")
    return {
        "title": poem_title,
        "chunks": chunks
    }
def rag_query(query, collection):
    

    print(f"\nðŸ” Query: {query}")

    # ------------------------------------------------
    # 1. Detect poem using your function
    # ------------------------------------------------
    start_time = time.time()
    poem_title = detect_poem_title(query, collection)
    detection_time = time.time() - start_time
    print(f"â±ï¸ Detection took: {detection_time:.2f}s")

    # ------------------------------------------------
    # 2. Retrieve all chunks using your function
    # ------------------------------------------------
    chunks = get_poem_chunks(poem_title, collection)
    print(f"ðŸ“š Retrieved {len(chunks)} chunks for poem '{poem_title}'")

    # Join chunks into one context string
    context = "\n\n".join(chunks)
    print(f"ðŸ“ Total context length: {len(context)} characters")

    # ------------------------------------------------
    # 3. Build LLM prompt (improved)
    # ------------------------------------------------
    # === Prompt ===
    system_prompt = """
Ø¢Ù¾ Ø§Ø±Ø¯Ùˆ Ø§Ø¯Ø¨ Ø§ÙˆØ± Ø¨Ø§Ù„Ù Ø¬Ø¨Ø±Ø¦ÛŒÙ„ / Ø¨Ø§Ù†Ú¯Ù Ø¯Ø±Ø§ Ú©Û’ Ù…Ø§ÛØ± ÛÛŒÚºÛ”
Ø¯ÛŒÛ’ Ú¯Ø¦Û’ Ú©ÙˆÙ†Ù¹ÛŒÚ©Ø³Ù¹ (ØªØ´Ø±ÛŒØ­) Ú©ÛŒ Ù…Ø¯Ø¯ Ø³Û’ ØµØ§Ø±Ù Ú©Û’ Ø³ÙˆØ§Ù„ Ú©Ø§ Ø¨ÛØªØ±ÛŒÙ† Ù…Ù…Ú©Ù†Û Ø¬ÙˆØ§Ø¨ Ø¯ÛŒÚºÛ”

â€” ÛÙ…ÛŒØ´Û ØµØ§ÙØŒ Ø®ÙˆØ¨ØµÙˆØ±Øª Ø§Ø±Ø¯Ùˆ Ù…ÛŒÚº Ù„Ú©Ú¾ÛŒÚº
â€” ØµØ±Ù ÙØ±Ø§ÛÙ… Ú©Ø±Ø¯Û ØªØ´Ø±ÛŒØ­ Ø³Û’ Ø¬ÙˆØ§Ø¨ Ø¨Ù†Ø§Ø¦ÛŒÚº
â€” Ø¨Û’ ÙˆØ¬Û Ù„Ù…Ø¨ÛŒ ØªÚ©Ø±Ø§Ø± Ù†Û Ú©Ø±ÛŒÚº
â€” Ù…ÙÛÙˆÙ…ØŒ Ù¾ÛŒØºØ§Ù…ØŒ Ø§ÙˆØ± ÙÙ„Ø³ÙÛ ÙˆØ§Ø¶Ø­ Ú©Ø±ÛŒÚº
"""

    user_prompt = f"""
Ø³ÙˆØ§Ù„:
{query}

ØªØ´Ø±ÛŒØ­ (Context):
{context}

Ø¨Ø±Ø§Û Ú©Ø±Ù… Ø§Ø³ Ú©ÛŒ ÙˆØ¶Ø§Ø­Øª Ø§Ø±Ø¯Ùˆ Ù…ÛŒÚº Ú©Ø±ÛŒÚº:
"""

    # === Send to ALIF via LM Studio ===
    print("\nðŸ¤– Sending to Alif (LM Studio)...")

    start_time = time.time()
    
    response = requests.post(
        "http://localhost:1234/v1/chat/completions",  # or use your LAN IP
        json={
            "model": "alif-1.0-8b-instruct",
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ]
        }
    ).json()

    llm_time = time.time() - start_time
    print(f"â±ï¸  LLM took: {llm_time:.2f}s")
    
    answer = response['choices'][0]['message']['content']
    print(f"\nâœ… Response: {answer[:200]}...\n")
    
    return answer
